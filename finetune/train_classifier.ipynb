{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ee441d-7f0d-466a-8c25-e2d78f080d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"/mimer/NOBACKUP/groups/drl_mps_planner/agents/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc39ddb-8624-4daf-a277-8733a36b03b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    NVIDIA H100 NVL. Num GPUs = 1. Max memory: 93.086 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cb5c0aac4640e08889f9f12b5ecd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "torch._dynamo.reset()\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "\n",
    "model_id = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162e949f-4cbe-4eb9-b4a0-899322698157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.utils import is_torchdynamo_compiling\n",
    "import torch\n",
    "\n",
    "def embed_image_and_text(\n",
    "    base_model,\n",
    "    input_ids: Optional[torch.LongTensor] = None,  # text inputs\n",
    "    pixel_values: Optional[torch.FloatTensor] = None,  # vision inputs\n",
    "    input_features: Optional[torch.FloatTensor] = None,  # audio inputs\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    input_features_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n",
    "    token_type_ids: Optional[torch.LongTensor] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    image_features: Optional[torch.Tensor] = None,\n",
    "    **lm_kwargs,\n",
    ") -> torch.Tensor:\n",
    "    self = base_model\n",
    "    if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "        raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "\n",
    "    if input_ids is not None:\n",
    "        inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # Prepare per-layer inputs from inputs_ids\n",
    "        per_layer_inputs_mask = torch.logical_and(input_ids >= 0, input_ids < self.model.vocab_size_per_layer_input)\n",
    "        per_layer_inputs_tokens = torch.where(per_layer_inputs_mask, input_ids, torch.zeros_like(input_ids))\n",
    "        per_layer_inputs = self.language_model.get_per_layer_inputs(per_layer_inputs_tokens)\n",
    "\n",
    "        # Handle vision tokens (>= embed_vision.vocab_offset and < embed_audio.vocab_offset)\n",
    "        vision_mask = torch.logical_and(\n",
    "            input_ids >= self.model.embed_vision.vocab_offset, input_ids < self.model.embed_audio.vocab_offset\n",
    "        )\n",
    "        dummy_vision_token_id = self.model.embed_vision.vocab_offset + self.model.embed_vision.vocab_size - 1\n",
    "        vision_input_ids = torch.where(vision_mask, input_ids, dummy_vision_token_id).to(inputs_embeds.device)\n",
    "        vision_embeds = self.model.embed_vision(input_ids=vision_input_ids)\n",
    "        expanded_vision_mask = vision_mask.unsqueeze(-1).expand_as(inputs_embeds)\n",
    "        inputs_embeds = torch.where(expanded_vision_mask, vision_embeds, inputs_embeds)\n",
    "\n",
    "        # Handle audio tokens (>= embed_audio.vocab_offset)\n",
    "        audio_mask = input_ids >= self.model.embed_audio.vocab_offset\n",
    "        dummy_audio_token_id = self.model.embed_audio.vocab_offset + self.model.embed_audio.vocab_size - 1\n",
    "        audio_input_ids = torch.where(audio_mask, input_ids, dummy_audio_token_id).to(inputs_embeds.device)\n",
    "        audio_embeds = self.model.embed_audio(input_ids=audio_input_ids)\n",
    "        expanded_audio_mask = audio_mask.unsqueeze(-1).expand_as(inputs_embeds)\n",
    "        inputs_embeds = torch.where(expanded_audio_mask, audio_embeds, inputs_embeds)\n",
    "    else:\n",
    "        per_layer_inputs = None\n",
    "\n",
    "    # Merge text and images\n",
    "    if pixel_values is not None:\n",
    "        if image_features is None:\n",
    "            image_features = self.model.get_image_features(pixel_values)\n",
    "\n",
    "        if input_ids is None:\n",
    "            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n",
    "                torch.tensor(self.model.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n",
    "            )\n",
    "        else:\n",
    "            special_image_mask = (input_ids == self.model.config.image_token_id).unsqueeze(-1)\n",
    "            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n",
    "\n",
    "        if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n",
    "            image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n",
    "            raise ValueError(\n",
    "                f\"Number of images does not match number of special image tokens in the input text. \"\n",
    "                f\"Got {image_tokens_in_text} image tokens in the text and \"\n",
    "                f\"{image_features.shape[0] * image_features.shape[1]} tokens from image embeddings.\"\n",
    "            )\n",
    "        image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n",
    "    outputs = self.language_model(\n",
    "        input_ids=None,\n",
    "        per_layer_inputs=per_layer_inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=True,\n",
    "        cache_position=cache_position,\n",
    "        **lm_kwargs,\n",
    "    )\n",
    "    # returns embeddings before language model and after language model\n",
    "    return inputs_embeds, outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf119782-ee96-456f-a324-c215cfc94c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e6476e-92c4-4018-adea-2363e2fdeae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Where is the cat standing?\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(images=image, text=prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c3b512-1540-40c1-a42b-98de5c419400",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    early_embeddings, late_embeddings = embed_image_and_text(base_model=model, **inputs)#, image_features=image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339198dc-0eba-4472-a697-7743ee48e22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 275, 2048])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f51251-88ee-47e4-8b91-20aaedae1cc6",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9c3b71-d2ac-42a6-9504-efade0761cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_match_prompt(question: str):\n",
    "    return f\"\"\"# General Instructions\n",
    "You are given an image and a question and should indicate whether question about the image is true, wrong or irrelevant.\n",
    "\n",
    "# Labels:\n",
    "- \"true\": The image clearly supports the question—visual evidence is unambiguous.\n",
    "- \"false\": The image clearly contradicts the question—visual evidence directly disproves it.\n",
    "- \"null\": The image does not provide enough visual information to answer definitively.\n",
    "\n",
    "# Instructions:\n",
    "Use only the visual content of the image. Do not make assumptions. Choose \"null\" when there's not enough evidence, even if a guess seems likely.\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "# Output format:\n",
    "{{\"answer\": \"<true|false|null>\"}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0315a58-10f4-4ff8-a476-ced1922dd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "CACHE_DIR = \"image_cache\"\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "def download_image(url):\n",
    "    # Create a unique filename based on the URL hash\n",
    "    url_hash = hashlib.sha256(url.encode()).hexdigest()\n",
    "    file_extension = os.path.splitext(url.split('?')[0])[-1] or '.jpg' # Default to .jpg if no extension\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"{url_hash}{file_extension}\")\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            return Image.open(cache_path)\n",
    "        except IOError as e:\n",
    "            print(f\"Warning: Could not open cached image {cache_path}: {e}\")\n",
    "            # If cache is corrupt, proceed to download\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        # Resize the image to max 512x512, preserving aspect ratio\n",
    "        # image.thumbnail((512, 512))\n",
    "        # Save the resized image to the cache\n",
    "        image.save(cache_path)\n",
    "        return image\n",
    "    except (requests.exceptions.RequestException, IOError) as e:\n",
    "        print(f\"Warning: Could not download image {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def data_generator(path: str):\n",
    "    \"\"\"Yields records from a JSONL file for memory-efficient loading.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            obj = json.loads(line)\n",
    "            question = obj.get(\"query\") or obj.get(\"question\")\n",
    "            image_url = obj.get(\"imageUrl\")\n",
    "            reply = obj.get(\"reply\")\n",
    "\n",
    "            image = download_image(image_url)\n",
    "\n",
    "            if not (question and image_url and reply and image):\n",
    "                continue\n",
    "\n",
    "            yield {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"prompt\": image_match_prompt(question)},\n",
    "                            {\"type\": \"text\", \"text\": question},\n",
    "                            {\"type\": \"image\", \"image\": image},\n",
    "                            {\"type\": \"url\", \"text\": image_url},\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": reply},\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "\n",
    "def create_dataset(data_path):\n",
    "    \"\"\"Create a HuggingFace dataset by converting to a simpler format first.\"\"\"\n",
    "    # First, collect all the data in the simple format\n",
    "    if isinstance(data_path, list):\n",
    "        # If data_path is a list, process all files and combine results\n",
    "        data_items = []\n",
    "        for path in data_path:\n",
    "            data_items.extend(list(data_generator(path)))\n",
    "    else:\n",
    "        # If data_path is a single path, process it normally\n",
    "        data_items = list(data_generator(data_path))    \n",
    "    # Convert to a flat structure that HuggingFace can handle\n",
    "    flat_data = []\n",
    "    for item in data_items:\n",
    "        # Store the complex structure as a single field and handle images separately\n",
    "        user_message = item[\"messages\"][0]\n",
    "        assistant_message = item[\"messages\"][1]\n",
    "        \n",
    "        # Extract text and image from user message\n",
    "        text_content = None\n",
    "        image_content = None\n",
    "        \n",
    "        for content in user_message[\"content\"]:\n",
    "            if content[\"type\"] == \"text\":\n",
    "                if \"prompt\" in content:\n",
    "                    prompt_content\n",
    "                else:\n",
    "                    text_content = content[\"text\"]\n",
    "            elif content[\"type\"] == \"image\":\n",
    "                image_content = content[\"image\"]\n",
    "            elif content[\"type\"] == \"url\":\n",
    "                image_url = content[\"text\"]\n",
    "        \n",
    "        raw_reply = assistant_message[\"content\"][0][\"text\"]\n",
    "        if raw_reply == \"doesn't match\":\n",
    "            reply = '{\"response\": \"false\"}'\n",
    "            label = 0\n",
    "        elif raw_reply == \"matches\":\n",
    "            reply = '{\"response\": \"true\"}'\n",
    "            label = 1\n",
    "        elif raw_reply == \"irrelevant\":\n",
    "            reply = '{\"response\": \"null\"}'\n",
    "            label = 2\n",
    "        else:\n",
    "            print(f\"{raw_reply} not found\")\n",
    "\n",
    "        flat_data.append({\n",
    "            \"user_text\": text_content,\n",
    "            \"user_image\": image_content,\n",
    "            \"assistant_text\": reply,\n",
    "            \"image_url\": image_url,\n",
    "            \"label\": label,\n",
    "            # \"original_messages\": item[\"messages\"]  # Keep original structure\n",
    "        })\n",
    "    \n",
    "    # Create dataset from the flat structure\n",
    "    dataset = Dataset.from_list(flat_data)\n",
    "    \n",
    "    # Add a method to get back the original format\n",
    "    def get_original_format(example):\n",
    "        return example[\"original_messages\"]\n",
    "    \n",
    "    dataset.get_original_format = get_original_format\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b736cf85-da5f-4ebd-b12b-d0d369274166",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./image-analysis-results.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 100\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m user_message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m \u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    102\u001b[0m         image_content \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "data_path = \"./image-analysis-results.jsonl\"\n",
    "dataset = create_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459dbcf-e7b1-4f2a-9aa8-b29fc5cb5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380cd375-1e34-4f38-b187-8b553ee8d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad9a9d-2084-46bf-a8c0-7b26c7beeda4",
   "metadata": {},
   "source": [
    "# Prompt eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e25d23b7-7bda-4c23-b15c-d2dfcaba4f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 6444\n",
      "Training set size: 5155\n",
      "Validation set size: 1289\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and validation sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "# instruction = \"You are an expert at understanding aparment images. Answer accurately.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    conversation = [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"user_text\"]},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"user_image\"]} ]\n",
    "        },\n",
    "        { \"role\" : \"assistant\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"assistant_text\"]} ]\n",
    "        },\n",
    "    ]\n",
    "    return { \"messages\" : conversation }\n",
    "\n",
    "\n",
    "train_converted_dataset = [convert_to_conversation(sample) for sample in train_dataset]\n",
    "val_converted_dataset = [convert_to_conversation(sample) for sample in val_dataset]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "97ccfb32-e95f-4063-95f4-440b850f8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def extract_classification(text):\n",
    "    \"\"\"\n",
    "    Extract classification from model output.\n",
    "    Returns 'true', 'false', 'null', or None if not found.\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Try to extract from JSON format first\n",
    "    try:\n",
    "        # Look for JSON-like patterns\n",
    "        json_match = re.search(r'\\{[^}]*\"response\"[^}]*:[^}]*\"([^\"]+)\"[^}]*\\}', text)\n",
    "        if json_match:\n",
    "            response = json_match.group(1).lower().strip()\n",
    "            if response in ['true', 'false', 'null']:\n",
    "                return response\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Direct string matching as fallback\n",
    "    if 'null' in text:\n",
    "        return 'null'\n",
    "    elif 'true' in text:\n",
    "        return 'true'\n",
    "    elif 'false' in text:\n",
    "        return 'false'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def calculate_accuracy_batched(model, tokenizer, val_converted_dataset, batch_size=8):\n",
    "    \"\"\"Calculate accuracy using batched inference with robust classification extraction.\"\"\"\n",
    "    total = len(val_converted_dataset)\n",
    "    \n",
    "    # Track different types of errors for debugging\n",
    "    stats = {\n",
    "        'correct': 0,\n",
    "        'wrong_classification': 0,\n",
    "        'unparseable': 0,\n",
    "        'by_class': {'true': {'correct': 0, 'total': 0}, \n",
    "                     'false': {'correct': 0, 'total': 0}, \n",
    "                     'null': {'correct': 0, 'total': 0}}\n",
    "    }\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    processed_count = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch_end = min(i + batch_size, total)\n",
    "        batch_samples = val_converted_dataset[i:batch_end]\n",
    "        \n",
    "        batch_inputs = []\n",
    "        expected_responses = []\n",
    "        valid_indices = []  # Track which samples were successfully tokenized\n",
    "        \n",
    "        # Tokenize each sample individually\n",
    "        for idx, sample in enumerate(batch_samples):\n",
    "            try:\n",
    "                messages = sample['messages']\n",
    "                \n",
    "                # Extract user message (first message, excluding assistant)\n",
    "                user_message = messages[0]  # This contains the user text and image\n",
    "                \n",
    "                # Extract expected response (assistant message content)\n",
    "                assistant_message = messages[1]  # This is the assistant response\n",
    "                expected_response = assistant_message['content'][0]['text']  # Extract the text\n",
    "                \n",
    "                # Tokenize individual sample\n",
    "                inputs = tokenizer.apply_chat_template(\n",
    "                    [user_message],\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                batch_inputs.append(inputs[0])  # Remove batch dimension\n",
    "                expected_responses.append(expected_response)\n",
    "                valid_indices.append(i + idx)  # Track original index\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error tokenizing sample {i + idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not batch_inputs:\n",
    "            continue\n",
    "            \n",
    "        # Manually pad to the same length (left padding)\n",
    "        max_length = max(len(inp) for inp in batch_inputs)\n",
    "        padded_inputs = []\n",
    "        original_lengths = []\n",
    "        \n",
    "        for inp in batch_inputs:\n",
    "            original_lengths.append(len(inp))\n",
    "            if len(inp) < max_length:\n",
    "                # Left pad with pad_token_id\n",
    "                padding_length = max_length - len(inp)\n",
    "                padded = torch.cat([\n",
    "                    torch.full((padding_length,), tokenizer.pad_token_id, dtype=inp.dtype),\n",
    "                    inp\n",
    "                ])\n",
    "                padded_inputs.append(padded)\n",
    "            else:\n",
    "                padded_inputs.append(inp)\n",
    "        \n",
    "        # Stack into batch tensor\n",
    "        input_ids = torch.stack(padded_inputs).to(model.device)\n",
    "        \n",
    "        # Create attention mask for left-padded sequences\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.0,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Process each sample in the batch\n",
    "        for j in range(len(batch_inputs)):\n",
    "            # Use the original length we stored\n",
    "            original_length = original_lengths[j]\n",
    "            \n",
    "            # For left-padded sequences, we need to account for the padding\n",
    "            padding_length = max_length - original_length\n",
    "            actual_input_end = padding_length + original_length\n",
    "            \n",
    "            # The generated part starts after the actual input\n",
    "            generated_tokens = outputs[j][actual_input_end:]\n",
    "            \n",
    "            # Decode the generated response\n",
    "            generated_text = tokenizer.decode(\n",
    "                generated_tokens, \n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            # Extract classifications\n",
    "            expected_class = extract_classification(expected_responses[j])\n",
    "            predicted_class = extract_classification(generated_text)\n",
    "            \n",
    "            if expected_class and predicted_class:\n",
    "                y_true.append(expected_class)\n",
    "                y_pred.append(predicted_class)\n",
    "            \n",
    "            # Update statistics\n",
    "            if expected_class:\n",
    "                stats['by_class'][expected_class]['total'] += 1\n",
    "            \n",
    "            if predicted_class is None:\n",
    "                stats['unparseable'] += 1\n",
    "                if processed_count < 10:  # Show first few for debugging\n",
    "                    print(f\"Unparseable output: '{generated_text}' (expected: '{expected_responses[j]}')\")\n",
    "            elif predicted_class == expected_class:\n",
    "                stats['correct'] += 1\n",
    "                if expected_class:\n",
    "                    stats['by_class'][expected_class]['correct'] += 1\n",
    "            else:\n",
    "                stats['wrong_classification'] += 1\n",
    "                if processed_count < 10:  # Show first few misclassifications for debugging\n",
    "                    print(f\"Misclassification: predicted '{predicted_class}', expected '{expected_class}'\")\n",
    "            \n",
    "            processed_count += 1\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Processed {processed_count}/{total} samples\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    accuracy = stats['correct'] / processed_count if processed_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Overall Accuracy: {stats['correct']}/{processed_count} = {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {stats['correct']}\")\n",
    "    print(f\"Wrong classifications: {stats['wrong_classification']}\")\n",
    "    print(f\"Unparseable outputs: {stats['unparseable']}\")\n",
    "    \n",
    "    print(f\"\\n=== BY CLASS ===\")\n",
    "    for class_name, class_stats in stats['by_class'].items():\n",
    "        if class_stats['total'] > 0:\n",
    "            class_acc = class_stats['correct'] / class_stats['total']\n",
    "            print(f\"{class_name.upper()}: {class_stats['correct']}/{class_stats['total']} = {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "    \n",
    "    # Generate and plot confusion matrix\n",
    "    if y_true and y_pred:\n",
    "        labels = sorted(list(set(y_true)))\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        print(\"\\nConfusion matrix plot saved to 'confusion_matrix.png'\")\n",
    "\n",
    "    return accuracy, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6fa3c2bd-0776-4e75-9aae-2da1a3bd291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set padding_side to 'left' for decoder-only model\n",
      "Misclassification: predicted 'null', expected 'false'\n",
      "Misclassification: predicted 'null', expected 'false'\n",
      "Misclassification: predicted 'null', expected 'false'\n",
      "Misclassification: predicted 'null', expected 'false'\n",
      "Misclassification: predicted 'false', expected 'null'\n",
      "Processed 64/1289 samples\n",
      "Processed 128/1289 samples\n",
      "Processed 192/1289 samples\n",
      "Processed 256/1289 samples\n",
      "Processed 320/1289 samples\n",
      "Processed 384/1289 samples\n",
      "Processed 448/1289 samples\n",
      "Processed 512/1289 samples\n",
      "Processed 576/1289 samples\n",
      "Processed 640/1289 samples\n",
      "Processed 704/1289 samples\n",
      "Processed 768/1289 samples\n",
      "Processed 832/1289 samples\n",
      "Processed 896/1289 samples\n",
      "Processed 960/1289 samples\n",
      "Processed 1024/1289 samples\n",
      "Processed 1088/1289 samples\n",
      "Processed 1152/1289 samples\n",
      "Processed 1216/1289 samples\n",
      "Processed 1280/1289 samples\n",
      "Processed 1289/1289 samples\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "Overall Accuracy: 606/1289 = 0.4701 (47.01%)\n",
      "Correct predictions: 606\n",
      "Wrong classifications: 683\n",
      "Unparseable outputs: 0\n",
      "\n",
      "=== BY CLASS ===\n",
      "TRUE: 19/54 = 0.3519 (35.19%)\n",
      "FALSE: 28/521 = 0.0537 (5.37%)\n",
      "NULL: 559/714 = 0.7829 (78.29%)\n",
      "\n",
      "Confusion matrix plot saved to 'confusion_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "# Ensure pad token is properly set and configure padding side\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set pad_token to eos_token\")\n",
    "\n",
    "# Set left padding for decoder-only models\n",
    "tokenizer.padding_side = 'left'\n",
    "print(\"Set padding_side to 'left' for decoder-only model\")\n",
    "\n",
    "# Calculate accuracy on validation set\n",
    "accuracy, detailed_stats = calculate_accuracy_batched(model, tokenizer, val_converted_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e58ebf-9c67-4d31-aca8-fcda03bf743f",
   "metadata": {},
   "source": [
    "# Embed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0517728-a2bb-42e4-a9cb-68f04c30cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57268885-478e-408a-8661-31cfc95946d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gemma3nForConditionalGeneration' object has no attribute 'embed_image_and_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_image_features(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 20\u001b[0m     embeddings, output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_image_and_text\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, image_features\u001b[38;5;241m=\u001b[39mimage_features)\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/drl_mps_planner/agents/agents_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Gemma3nForConditionalGeneration' object has no attribute 'embed_image_and_text'"
     ]
    }
   ],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Where is the cat standing?\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "image_features = model.get_image_features(inputs[\"pixel_values\"])\n",
    "\n",
    "with torch.inference_mode():\n",
    "    embeddings, output = embed_image_and_text(base_model=model, **inputs, image_features=image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86ba4968-95fe-4b0d-a622-3846db270139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6444/6444 [09:51<00:00, 10.90it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_cache = {}\n",
    "embeddings = []\n",
    "embeddings_1 = []\n",
    "with torch.inference_mode():\n",
    "    for sample in tqdm(dataset):\n",
    "        url = sample[\"image_url\"]\n",
    "        image = sample[\"user_image\"]\n",
    "        #image = Image.open(requests.get(url, stream=True).raw)\n",
    "    \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": sample[\"user_text\"]},\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "        if url in embedding_cache:\n",
    "            image_features = embedding_cache[url]\n",
    "        else:\n",
    "            image_features = model.get_image_features(inputs[\"pixel_values\"])\n",
    "            embedding_cache[url] = image_features\n",
    "        embedding_1, embedding = embed_image_and_text(base_model=model, **inputs, image_features=image_features)\n",
    "        if embedding.isnan().any():\n",
    "            print(\"nan\")\n",
    "            break\n",
    "        embeddings.append(embedding.detach().clone().cpu())\n",
    "        embeddings_1.append(embedding_1.detach().clone().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a2502-5d2d-4189-b36e-0d68af8057e8",
   "metadata": {},
   "source": [
    "# Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e710cd41-836d-46ba-ba80-54eba299e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a2ae80-51dc-45fb-a4af-f0acd0058144",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065df3b8-281c-4de5-a8e8-3976c85b2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5260a-8ba2-4fc0-aa2b-88b047542541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements an attention mechanism that can handle padded sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embedding_dim // 2, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Padded input tensor of shape [batch_size, seq_len, embedding_dim]\n",
    "            attention_mask (torch.Tensor): Boolean mask of shape [batch_size, seq_len, 1]\n",
    "        Returns:\n",
    "            torch.Tensor: A pooled representation of shape [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        attention_scores = self.attention_net(x)\n",
    "\n",
    "        # --- MASKING STEP ---\n",
    "        # Where the mask is False (i.e., for padded elements), set scores to a very\n",
    "        # large negative number so they become zero after softmax.\n",
    "        attention_scores.masked_fill_(~attention_mask, -float('inf'))\n",
    "\n",
    "        # Convert scores to weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # --- FIX for NaN issue ---\n",
    "        # If a sequence is entirely padded, softmax(-inf) results in NaN.\n",
    "        # We replace any NaNs with 0.0 to ensure numerical stability.\n",
    "        attention_weights = torch.nan_to_num(attention_weights, nan=0.0)\n",
    "\n",
    "        # The weights for padded elements will be 0, so they don't contribute\n",
    "        # to the weighted average.\n",
    "        weighted_average = torch.sum(x * attention_weights, dim=1)\n",
    "\n",
    "        return weighted_average\n",
    "\n",
    "class EmbeddingClassifier(L.LightningModule):\n",
    "    \"\"\"\n",
    "    A classifier that uses an attention mechanism to pool variable-length sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=2048, hidden_dim=512, num_classes=3, learning_rate=1e-3, dropout_p=0.5, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.attention_pooling = AttentionPooling(embedding_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hparams.embedding_dim, self.hparams.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.hparams.dropout_p),\n",
    "            nn.Linear(self.hparams.hidden_dim, self.hparams.num_classes)\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=self.hparams.class_weights)\n",
    "        self.train_accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if self.hparams.class_weights is not None:\n",
    "            self.loss_fn.weight = self.hparams.class_weights.to(self.device)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        The forward pass now accepts the mask and passes it to the pooling layer.\n",
    "        \"\"\"\n",
    "        pooled_output = self.attention_pooling(x, attention_mask)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # The batch now contains embeddings, labels, and the attention mask\n",
    "        embeddings, labels, mask = batch\n",
    "        logits = self(embeddings, mask)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        self.train_accuracy(logits, labels)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        embeddings, labels, mask = batch\n",
    "        logits = self(embeddings, mask)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        self.val_accuracy(logits, labels)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_accuracy, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',      # Reduce on minimum validation loss\n",
    "            factor=0.2,      # new_lr = lr * factor\n",
    "            patience=2,      # Number of epochs with no improvement after which lr is reduced\n",
    "            min_lr=1e-6,     # Don't let the learning rate go too low\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\", # The metric to watch\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "974eb8f0-f4bc-4fbe-b62e-24d707a01a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for variable-length sequences.\n",
    "\n",
    "    Args:\n",
    "        features (list): A list of feature tensors. Each tensor in the list\n",
    "                         can have a variable sequence length,\n",
    "                         e.g., shape [seq_len, embedding_dim].\n",
    "        labels (list or torch.Tensor): A list or tensor of corresponding labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "        # Ensure features and labels have the same number of samples\n",
    "        assert len(features) == len(labels), \"Features and labels must have the same length\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the feature and label for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (feature_tensor, label_tensor)\n",
    "        \"\"\"\n",
    "        # Get the feature tensor at the given index\n",
    "        feature = self.features[idx].to(torch.float32)\n",
    "        # Get the label at the given index\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # It's good practice to ensure the label is a tensor\n",
    "        if not isinstance(label, torch.Tensor):\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "\n",
    "def collate_fn_pad(batch, target_embedding_dim=2048):\n",
    "    \"\"\"\n",
    "    This function is passed to the DataLoader to handle padding.\n",
    "    It takes a list of samples (tuples) and returns a single batch.\n",
    "    It also normalizes the embedding dimension to a fixed size to prevent errors.\n",
    "    \"\"\"\n",
    "    # 1. Separate embeddings and labels\n",
    "    embeddings_list = [item[0] for item in batch]\n",
    "    labels_list = [item[1] for item in batch]\n",
    "\n",
    "    # --- NEW: Normalize embedding dimension to a fixed size ---\n",
    "    # This loop fixes the error by ensuring all tensors have the same embedding dim.\n",
    "    normalized_embeddings_list = []\n",
    "    for emb in embeddings_list:\n",
    "        # --- FIX: Handle potential extra batch dimension ---\n",
    "        if emb.dim() == 3 and emb.shape[0] == 1:\n",
    "            emb = emb.squeeze(0)\n",
    "\n",
    "        # Use the last dimension as the embedding dimension for robustness\n",
    "        current_dim = emb.shape[-1]\n",
    "\n",
    "        if current_dim > target_embedding_dim:\n",
    "            # Truncate the embedding dimension if it's too large\n",
    "            normalized_emb = emb[:, :target_embedding_dim]\n",
    "        elif current_dim < target_embedding_dim:\n",
    "            # Pad the embedding dimension with zeros if it's too small\n",
    "            padding_size = target_embedding_dim - current_dim\n",
    "            padding = torch.zeros((emb.shape[0], padding_size), dtype=emb.dtype, device=emb.device)\n",
    "            normalized_emb = torch.cat([emb, padding], dim=1)\n",
    "        else:\n",
    "            # No change needed if the dimension is correct\n",
    "            normalized_emb = emb\n",
    "        normalized_embeddings_list.append(normalized_emb)\n",
    "\n",
    "    # 2. Pad the sequences of embeddings using the normalized list\n",
    "    # `pad_sequence` stacks them and pads with 0 to the length of the longest sequence.\n",
    "    # batch_first=True makes the output shape [batch_size, seq_len, embedding_dim]\n",
    "    padded_embeddings = pad_sequence(normalized_embeddings_list, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # 3. Create the attention mask\n",
    "    # The mask is True where there is real data, False for padding (0.0)\n",
    "    # We check the sum across the embedding dimension. If it's 0, it's padding.\n",
    "    attention_mask = (padded_embeddings.sum(dim=2) != 0).unsqueeze(2)\n",
    "\n",
    "    # 4. Stack the labels into a tensor\n",
    "    labels = torch.stack(labels_list)\n",
    "\n",
    "    return padded_embeddings, labels, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1a2e8d9-9e58-4f46-b5f2-f7b4e5444d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "vision_text_embeddings = copy.deepcopy(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adaea4f6-9b20-4294-9bbb-eb36ee0e18f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vision_text_embeddings[110]==vision_text_embeddings[6000]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fecc5a90-f4c9-46b4-8ed3-a0a133d6b45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.5625, 13.1875, 11.1875,  ...,  7.2188,  8.6250,  7.7812]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_text_embeddings[1].max(dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e9f2acd-55fc-40a6-aa3c-dec1aead0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#mean_pool_embeddings = [e.nanmean(dim=1) for e in vision_text_embeddings]\n",
    "all_labels = torch.tensor(dataset[\"label\"], dtype=torch.long)\n",
    "mean_embeddings = torch.cat([e.mean(dim=1) for e in vision_text_embeddings]).to(torch.float32)\n",
    "max_embeddings = torch.cat([e.max(dim=1).values for e in vision_text_embeddings]).to(torch.float32)\n",
    "min_embeddings = torch.cat([e.min(dim=1).values for e in vision_text_embeddings]).to(torch.float32)\n",
    "all_embeddings = torch.cat([mean_embeddings, max_embeddings, min_embeddings], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4e30eee-1072-4446-9f33-ab9e27b5c265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6444, 6144])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33991651-bdd0-4380-9d17-7bf3e4e893f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    vision_text_embeddings,\n",
    "    all_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c84dd1d-1e61-4956-91ef-228914652b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a49bb8d-2e57-40dc-828f-86b726077c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before Oversampling ---\n",
      "Class 0: 2185 samples\n",
      "Class 1: 193 samples\n",
      "Class 2: 2777 samples\n",
      "Majority class count: 2777\n",
      "\n",
      "Oversampling Class 0: Adding 493 samples...\n",
      "Oversampling Class 1: Adding 2153 samples...\n",
      "\n",
      "--- After Oversampling ---\n",
      "Class 0: 2678 samples\n",
      "Class 1: 2346 samples\n",
      "Class 2: 2777 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Before Oversampling ---\")\n",
    "# Get the unique classes and their counts in the training set\n",
    "unique_classes, class_counts = np.unique(train_labels.numpy(), return_counts=True)\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "# Find the majority class count\n",
    "majority_count = class_counts.max()\n",
    "print(f\"Majority class count: {majority_count}\\n\")\n",
    "\n",
    "# Convert features to a list if they are not already\n",
    "# (This makes appending easier)\n",
    "train_labels_list = train_labels.tolist()\n",
    "train_embeddings_list = train_embeddings\n",
    "\n",
    "\n",
    "# Loop through each class to perform oversampling\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    if count < majority_count:\n",
    "        # Calculate how many samples we need to add\n",
    "        num_to_add = int((majority_count - count) // 1.2)\n",
    "        print(f\"Oversampling Class {cls}: Adding {num_to_add} samples...\")\n",
    "\n",
    "        # Get the indices of all samples belonging to the current minority class\n",
    "        minority_indices = np.where(train_labels.numpy() == cls)[0]\n",
    "\n",
    "        # Randomly choose indices from the minority class to duplicate\n",
    "        # `replace=True` allows us to pick the same sample multiple times\n",
    "        indices_to_add = np.random.choice(minority_indices, size=num_to_add, replace=True)\n",
    "\n",
    "        # Add the chosen samples to our training lists\n",
    "        for idx in indices_to_add:\n",
    "            train_embeddings_list.append(train_embeddings[idx])\n",
    "            train_labels_list.append(train_labels[idx])\n",
    "\n",
    "print(\"\\n--- After Oversampling ---\")\n",
    "# Verify the new class distribution\n",
    "new_labels_tensor = torch.tensor(train_labels_list)\n",
    "unique_classes, class_counts = np.unique(new_labels_tensor.numpy(), return_counts=True)\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "126b010d-a67b-41ce-8c0b-c769cf2adc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = FeatureDataset(features=train_embeddings_list, labels=train_labels_list)\n",
    "val_dataset = FeatureDataset(features=val_embeddings, labels=val_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_pad\n",
    ")\n",
    "#\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn_pad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b2f0bed0-574e-4841-87b8-cc5ebc90c5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated class weights: tensor([0.9710, 1.1084, 0.9364])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels_list),\n",
    "    y=np.array(train_labels_list)\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(f\"Calculated class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2977f622-7562-4aa1-93e1-a394772b2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b247c009-a5ab-4d9f-b709-f200151053eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "94b77c60-3bbb-4393-ae9e-9dc9960d89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n",
      "/mimer/NOBACKUP/groups/drl_mps_planner/agents/agents_venv/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /apps/Arch/software/jupyter-server/2.7.2-GCCcore-12. ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(42)\n",
    "\n",
    "EMBEDDING_DIM = 1*2048\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "classifier = EmbeddingClassifier(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=1e-4,\n",
    "    dropout_p=0.4,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='checkpoints',\n",
    "    filename='best-model-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "logger = WandbLogger(log_model=\"none\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto',\n",
    "    logger=logger,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80fb168d-c2e8-4e21-9cd8-c20c5bba5a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mimer/NOBACKUP/groups/drl_mps_planner/agents/agents_venv/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/mimer/NOBACKUP/groups/drl_mps_planner/agents/agents_venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /mimer/NOBACKUP/groups/drl_mps_planner/agents/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type               | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | attention_pooling | AttentionPooling   | 2.1 M  | train\n",
      "1 | classifier        | Sequential         | 1.1 M  | train\n",
      "2 | loss_fn           | CrossEntropyLoss   | 0      | train\n",
      "3 | train_accuracy    | MulticlassAccuracy | 0      | train\n",
      "4 | val_accuracy      | MulticlassAccuracy | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.599    Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mimer/NOBACKUP/groups/drl_mps_planner/agents/agents_venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/mimer/NOBACKUP/groups/drl_mps_planner/agents/agents_venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949096bfb91f413f8785da2770a8495c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37855d668a69482b872dd28bb6dcaf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d05ea150bf94a5193c0443feac61b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf4c54e844e4b918d0c6bc6f35abfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42da7341b68041f49c1cc5420194df58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(classifier, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5403d6b-e5f0-4a1c-838f-8e825597523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the best model: /mimer/NOBACKUP/groups/drl_mps_planner/agents/checkpoints/best-model-epoch=22-val_acc=0.85.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingClassifier(\n",
       "  (attention_pooling): AttentionPooling(\n",
       "    (attention_net): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=1024, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (train_accuracy): MulticlassAccuracy()\n",
       "  (val_accuracy): MulticlassAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = model_checkpoint_callback.best_model_path\n",
    "print(f\"Path to the best model: {best_model_path}\")\n",
    "\n",
    "best_model = EmbeddingClassifier.load_from_checkpoint(best_model_path)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1fb5ec73-8ee7-4e37-b685-43cafb32ffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mimer/NOBACKUP/groups/drl_mps_planner/agents/agents_venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:149: `.validate(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.validate(ckpt_path='best')` to use the best model or `.validate(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /mimer/NOBACKUP/groups/drl_mps_planner/agents/checkpoints/best-model-epoch=22-val_acc=0.85.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /mimer/NOBACKUP/groups/drl_mps_planner/agents/checkpoints/best-model-epoch=22-val_acc=0.85.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running test on validation data (as a proxy for a test set)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14972fc40b641c5b626a3c28de056c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8487199544906616     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5024335980415344     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8487199544906616    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5024335980415344    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.5024335980415344, 'val_acc': 0.8487199544906616}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nRunning test on validation data (as a proxy for a test set)...\")\n",
    "trainer.validate(dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f013018a-76e2-4697-9687-634f42aa2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for the validation set...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --- Generate Predictions for the Entire Validation Set ---\n",
    "print(\"Generating predictions for the validation set...\")\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Use the val_loader for efficient batch processing\n",
    "for batch in val_loader:\n",
    "    embeddings, labels, mask = batch\n",
    "    # Move data to the same device as the model if using a GPU\n",
    "    embeddings = embeddings.to(best_model.device)\n",
    "    labels = labels.to(best_model.device)\n",
    "    mask = mask.to(best_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = best_model(embeddings, mask)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Append batch predictions and labels to our lists\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1221b033-e155-4bc2-968f-78a326af1aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting the confusion matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Plotting the confusion matrix...\")\n",
    "\n",
    "# Define the class names for the plot labels\n",
    "class_names = ['False', 'True', 'Null']\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotting the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True,     # Show the numbers in the cells\n",
    "    fmt='d',        # Format as integers\n",
    "    cmap='Blues',   # Color scheme\n",
    "    xticklabels=class_names, \n",
    "    yticklabels=class_names\n",
    ")\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(\"./conf_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f38d139d-1e2d-4258-92e9-6815f31be3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for the plot\n",
    "labels = ['Prompt Based', 'Classification']\n",
    "accuracies = [47, 85]\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "# Create the bar chart\n",
    "bars = ax.bar(labels, accuracies, color=['skyblue', 'steelblue'])\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_ylim(0, 100) # Set y-axis to go from 0 to 100 for percentage context\n",
    "\n",
    "# Add the accuracy value on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2.0, yval + 2, f'{yval}%', ha='center', va='bottom')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./acc_bar.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
